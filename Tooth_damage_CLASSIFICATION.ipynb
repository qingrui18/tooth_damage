{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1P9t45uxkt7DLYB86tXgoj_olIJJNzUyx","authorship_tag":"ABX9TyPL8ObVPfDp/wgyqQDph6Tu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from torchvision import transforms, datasets, models\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 数据预处理\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((256,256)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(128),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# 数据集路径\n","data_dir = '/content/drive/MyDrive/TOOTH_DAMAGE_CALSSIFICATION'\n","image_datasets = datasets.ImageFolder(data_dir, data_transforms['train'])\n","\n","# 定义模型\n","def get_model(model_name):\n","    if model_name == 'resnet':\n","        model = models.resnet18(pretrained=True)\n","        num_ftrs = model.fc.in_features\n","        model.fc = nn.Linear(num_ftrs, len(image_datasets.classes))\n","    elif model_name == 'efficientnet':\n","        model = models.efficientnet_b3(pretrained=True)\n","        num_ftrs = model.classifier[1].in_features\n","        model.classifier[1] = nn.Linear(num_ftrs, len(image_datasets.classes))\n","    elif model_name == 'densenet':\n","        model = models.densenet161(pretrained=True)\n","        num_ftrs = model.classifier.in_features\n","        model.classifier = nn.Linear(num_ftrs, len(image_datasets.classes))\n","    else:\n","        raise ValueError('Invalid model name')\n","    return model\n","\n","# 训练和验证\n","def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=25):\n","    best_model_wts = model.state_dict()\n","    best_acc = 0.0\n","\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    for epoch in range(num_epochs):\n","        epoch_train_loss = 0.0\n","        epoch_val_loss = 0.0\n","        epoch_train_corrects = 0\n","        epoch_val_corrects = 0\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            if phase == 'train':\n","                train_losses.append(epoch_loss)\n","                train_accuracies.append(epoch_acc.item())\n","                epoch_train_loss = epoch_loss\n","                epoch_train_corrects = running_corrects\n","            else:\n","                val_losses.append(epoch_loss)\n","                val_accuracies.append(epoch_acc.item())\n","                epoch_val_loss = epoch_loss\n","                epoch_val_corrects = running_corrects\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = model.state_dict()\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print(f'Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_corrects.double()/len(dataloaders[\"train\"].dataset):.4f}')\n","        print(f'Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_corrects.double()/len(dataloaders[\"val\"].dataset):.4f}')\n","\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, train_losses, val_losses, train_accuracies, val_accuracies\n","\n","# 交叉验证和评估\n","def cross_validate(model_name, num_epochs=25, batch_size=32):\n","    kf = KFold(n_splits=5, shuffle=True, random_state= 42)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    all_labels = np.array(image_datasets.targets)\n","    all_preds = np.zeros_like(all_labels)\n","    error_samples = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(kf.split(image_datasets)):\n","        print(f'Fold {fold + 1}')\n","\n","        train_subset = Subset(image_datasets, train_idx)\n","        val_subset = Subset(image_datasets, val_idx)\n","\n","        dataloaders = {\n","            'train': DataLoader(train_subset, batch_size=batch_size, shuffle=True),\n","            'val': DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n","        }\n","\n","        model = get_model(model_name)\n","        model = model.to(device)\n","\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n","\n","        model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, dataloaders, device, num_epochs)\n","\n","        val_preds = []\n","        val_labels = []\n","\n","        # 预测\n","        model.eval()\n","        with torch.no_grad():\n","            for inputs, labels in dataloaders['val']:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(inputs)\n","                _, preds = torch.max(outputs, 1)\n","\n","                val_preds.extend(preds.cpu().numpy())\n","                val_labels.extend(labels.cpu().numpy())\n","\n","                # 收集错误预测的样本\n","                error_samples.extend([(inputs[j], preds[j], labels[j]) for j in range(len(preds)) if preds[j] != labels[j]])\n","\n","        all_preds[val_idx] = val_preds\n","\n","        plt.figure(figsize=(10, 4))\n","        plt.subplot(1, 2, 1)\n","        plt.plot(train_losses, label='Train Loss')\n","        plt.plot(val_losses, label='Val Loss')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.title(f'{model_name} - Fold {fold+1} Loss')\n","\n","        plt.subplot(1, 2, 2)\n","        plt.plot(train_accuracies, label='Train Accuracy')\n","        plt.plot(val_accuracies, label='Val Accuracy')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Accuracy')\n","        plt.legend()\n","        plt.title(f'{model_name} - Fold {fold+1} Accuracy')\n","\n","        plt.show()\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'F1 Score: {f1:.4f}')\n","\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=image_datasets.classes, yticklabels=image_datasets.classes)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.show()\n","\n","\n","    # 显示错误预测的图片\n","    fig, axes = plt.subplots(len(error_samples), 1, figsize=(12, len(error_samples) * 4))\n","    fig.suptitle('Sample Misclassified Images')\n","    for i, (img, pred, label) in enumerate(error_samples[:len(error_samples)]):\n","        img = img.cpu().numpy().transpose((1, 2, 0))\n","        img = np.clip(img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406], 0, 1)\n","        axes[i].imshow(img)\n","        axes[i].set_title(f'Pred: {image_datasets.classes[pred]} / True: {image_datasets.classes[label]}')\n","        axes[i].axis('off')\n","    plt.show()\n","\n","# 执行\n","for model_name in ['resnet']:\n","    print(f'\\nTraining {model_name} model:')\n","    cross_validate(model_name, num_epochs=35, batch_size=8)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_rx3ZURjKH3oIjGkPC5NrqJ4FnczsKPb"},"id":"WWgB0IyjUj0r","executionInfo":{"status":"ok","timestamp":1717465207592,"user_tz":-600,"elapsed":384188,"user":{"displayName":"QINGRUI LI","userId":"18412504899692928229"}},"outputId":"a0b9e72e-0c29-41bf-a63a-6b5c471bef7b"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}